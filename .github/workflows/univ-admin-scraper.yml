name: University Admissions Scraper

on:
  # Run on schedule (once a week on Monday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 1'
  
  # Run when changes are made to key files
  push:
    branches:
      - main
    paths:
      - 'data/list_of_universities.txt'
      - 'university_list_processor.py'
      - 'main.py'
      - '.github/workflows/univ-admin-scraper.yml'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for running scraper'
        required: false
        default: 'Manual trigger'

jobs:
  process-university-list:
    name: Process University List
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Install dependencies for AI agent
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4
          
      - name: Extract config values
        id: config
        run: |
          # Use Python to extract values from config.py
          UNIVERSITIES_LIST_FILE=$(python -c "import config; print(config.UNIVERSITIES_LIST_FILE)")
          INPUT_FILE=$(python -c "import config; print(config.INPUT_FILE)")
          echo "universities_list_file=$UNIVERSITIES_LIST_FILE" >> $GITHUB_OUTPUT
          echo "input_file=$INPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Check for university list
        id: check-list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          if [ -f "$UNIVERSITIES_LIST_FILE" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "List of universities found at $UNIVERSITIES_LIST_FILE"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: No $UNIVERSITIES_LIST_FILE file found. Please create this file with a list of universities, one per line."
            exit 1
          fi
      
      - name: Verify AI agent script
        id: check-agent
        run: |
          if [ -f "university_list_processor.py" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "AI agent script found"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: university_list_processor.py not found. Please add the AI agent script to your repository."
            exit 1
          fi
      
      - name: Run AI agent to process university list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          INPUT_FILE="${{ steps.config.outputs.input_file }}"
          mkdir -p data
          python university_list_processor.py --input "$UNIVERSITIES_LIST_FILE" --output "$INPUT_FILE"
          
      - name: Display generated universities.json
        run: |
          echo "Generated universities.json:"
          cat "${{ steps.config.outputs.input_file }}"

  scrape-universities:
    name: Scrape University Admissions Pages
    needs: process-university-list
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Install minimal dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          python -m pip install -r requirements-minimal.txt
          
      - name: Verify dependencies
        run: |
          python -c "import aiohttp, bs4; print('Dependencies verified successfully')"
          
      - name: Extract config values
        id: config
        run: |
          # Use Python to extract values from config.py
          INPUT_FILE=$(python -c "import config; print(config.INPUT_FILE)")
          OUTPUT_FILE=$(python -c "import config; print(config.OUTPUT_FILE)")
          echo "input_file=$INPUT_FILE" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Run scraper
        run: |
          # Create data directory if it doesn't exist
          mkdir -p data
          
          # Run the scraper
          python main.py
          
      - name: Check and display scraping results
        run: |
          OUTPUT_FILE="${{ steps.config.outputs.output_file }}"
          if [ -f "$OUTPUT_FILE" ]; then
            echo "Scraping results:"
            cat "$OUTPUT_FILE"
            
            echo -e "\n\nUniversities successfully scraped:"
            python -c "import json; data = json.load(open('$OUTPUT_FILE')); print('\n'.join([f\"- {uni['name']}\" for uni in data]))"
          else
            echo "Error: No scraping results found in $OUTPUT_FILE"
            exit 1
          fi

      # Optional: Add email notification
      # - name: Send email notification
      #   if: ${{ always() }}
      #   uses: dawidd6/action-send-mail@v3
      #   with:
      #     server_address: ${{ secrets.MAIL_SERVER }}
      #     server_port: ${{ secrets.MAIL_PORT }}
      #     username: ${{ secrets.MAIL_USERNAME }}
      #     password: ${{ secrets.MAIL_PASSWORD }}
      #     subject: University Scraper Results - ${{ steps.summary.outputs.count }} Universities
      #     to: your-email@example.com
      #     from: GitHub Actions <actions@github.com>
      #     body: |
      #       The university admissions scraper has completed.
      #       ${{ steps.summary.outputs.count }} universities were successfully scraped.
      #       See the latest data in the repository: https://github.com/${{ github.repository }}/blob/main/data/admissions_data.json
      
      # Optional: Add Slack notification
      # - name: Send Slack notification
      #   uses: 8398a7/action-slack@v3
      #   with:
      #     status: ${{ job.status }}
      #     fields: repo,message,workflow,job,commit
      #     text: |
      #       Scraper completed: ${{ steps.summary.outputs.count }} universities were scraped.
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
