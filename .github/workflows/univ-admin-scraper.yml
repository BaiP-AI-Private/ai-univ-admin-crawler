name: University Admissions Scraper

on:
  # Run on schedule (once a week on Monday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 1'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for running scraper'
        required: false
        default: 'Manual trigger'

jobs:
  scrape:
    name: Scrape University Admissions Pages
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          
      - name: Run scraper
        run: |
          python main.py
          
      - name: Check if data was collected
        id: check-data
        run: |
          if [ -f "data/admissions_data.json" ]; then
            FILESIZE=$(stat -c%s "data/admissions_data.json")
            echo "filesize=$FILESIZE" >> $GITHUB_OUTPUT
            if [ "$FILESIZE" -gt 100 ]; then
              echo "valid=true" >> $GITHUB_OUTPUT
            else
              echo "valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "valid=false" >> $GITHUB_OUTPUT
          fi
          
      - name: List found universities
        if: steps.check-data.outputs.valid == 'true'
        run: |
          echo "Universities data successfully collected:"
          python -c "import json; data = json.load(open('data/admissions_data.json')); print('\n'.join([f\"- {uni['name']}\" for uni in data]))"
          
      - name: Commit and push if data changed
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          git add data/admissions_data.json
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update admissions data [skip ci]" && git push)
          
      - name: Upload data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: admissions-data
          path: data/admissions_data.json
          retention-days: 30
          
  notify:
    name: Notify about scraper results
    needs: scrape
    runs-on: ubuntu-latest
    if: ${{ always() }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Download data artifact
        uses: actions/download-artifact@v4
        with:
          name: admissions-data
          path: ./data
          
      - name: Generate summary
        id: summary
        run: |
          if [ -f "data/admissions_data.json" ]; then
            UNIS=$(python -c "import json; print(len(json.load(open('data/admissions_data.json'))))")
            echo "count=$UNIS" >> $GITHUB_OUTPUT
            echo "Universities scraped: $UNIS"
          else
            echo "count=0" >> $GITHUB_OUTPUT
            echo "No data was collected"
          fi
          
      # Optional: Add email notification
      # - name: Send email notification
      #   if: ${{ always() }}
      #   uses: dawidd6/action-send-mail@v3
      #   with:
      #     server_address: ${{ secrets.MAIL_SERVER }}
      #     server_port: ${{ secrets.MAIL_PORT }}
      #     username: ${{ secrets.MAIL_USERNAME }}
      #     password: ${{ secrets.MAIL_PASSWORD }}
      #     subject: University Scraper Results - ${{ steps.summary.outputs.count }} Universities
      #     to: your-email@example.com
      #     from: GitHub Actions <actions@github.com>
      #     body: |
      #       The university admissions scraper has completed.
      #       ${{ steps.summary.outputs.count }} universities were successfully scraped.
      #       See the latest data in the repository: https://github.com/${{ github.repository }}/blob/main/data/admissions_data.json
      
      # Optional: Add Slack notification
      # - name: Send Slack notification
      #   uses: 8398a7/action-slack@v3
      #   with:
      #     status: ${{ job.status }}
      #     fields: repo,message,workflow,job,commit
      #     text: |
      #       Scraper completed: ${{ steps.summary.outputs.count }} universities were scraped.
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
