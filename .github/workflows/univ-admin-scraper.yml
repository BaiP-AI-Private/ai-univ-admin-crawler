name: University Admissions Scraper

on:
  # Run on schedule (once a week on Monday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 1'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for running scraper'
        required: false
        default: 'Manual trigger'

jobs:
  process-university-list:
    name: Process University List
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'
          
      - name: Install dependencies for AI agent
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4
          
      - name: Extract config values
        id: config
        run: |
          # Use Python to extract values from config.py
          UNIVERSITIES_LIST_FILE=$(python -c "import config; print(config.UNIVERSITIES_LIST_FILE)")
          INPUT_FILE=$(python -c "import config; print(config.INPUT_FILE)")
          echo "universities_list_file=$UNIVERSITIES_LIST_FILE" >> $GITHUB_OUTPUT
          echo "input_file=$INPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Check for university list
        id: check-list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          if [ -f "$UNIVERSITIES_LIST_FILE" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "List of universities found at $UNIVERSITIES_LIST_FILE"
            echo "First few lines of the file:"
            head -n 5 "$UNIVERSITIES_LIST_FILE"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: No $UNIVERSITIES_LIST_FILE file not found. Please create this file with university names and optional fallback URLs."
            exit 1
          fi
      
      - name: Verify AI agent script
        id: check-agent
        run: |
          if [ -f "university_list_processor.py" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "AI agent script found"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: university_list_processor.py not found. Please add the AI agent script to your repository."
            exit 1
          fi
      
      - name: Run AI agent to process university list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          INPUT_FILE="${{ steps.config.outputs.input_file }}"
          mkdir -p data
          python university_list_processor.py --input "$UNIVERSITIES_LIST_FILE" --output "$INPUT_FILE" --debug
          
      - name: Display generated universities.json
        run: |
          echo "Generated universities.json:"
          cat "${{ steps.config.outputs.input_file }}"
          
      # Upload the generated universities.json as an artifact to share between jobs
      - name: Upload universities.json
        uses: actions/upload-artifact@v4
        with:
          name: universities-json
          path: ${{ steps.config.outputs.input_file }}
          retention-days: 1

  scrape-universities:
    name: Scrape University Admissions Pages with Crawl4AI
    needs: process-university-list
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'
          
      # Download the universities.json file generated in the previous job
      - name: Download universities.json
        uses: actions/download-artifact@v4
        with:
          name: universities-json
          path: data
          
      # Set CI environment variable
      - name: Set CI environment variable
        run: echo "CI=true" >> $GITHUB_ENV
          
      # Install Rust for tokenizers
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true
          
      # Install system dependencies for Playwright
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            libglib2.0-0 \
            libnss3 \
            libnspr4 \
            libatk1.0-0 \
            libatk-bridge2.0-0 \
            libcups2 \
            libdrm2 \
            libdbus-1-3 \
            libxcb1 \
            libxkbcommon0 \
            libx11-6 \
            libxcomposite1 \
            libxdamage1 \
            libxext6 \
            libxfixes3 \
            libxrandr2 \
            libgbm1 \
            libpango-1.0-0 \
            libcairo2 \
            libasound2 \
            libatspi2.0-0
            
      # Install Python dependencies in a specific order to avoid conflicts
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          
          # Install minimal dependencies first (without transformers)
          pip install "pydantic>=2.10.0" "fastapi>=0.100.0" "crawl4ai==0.6.3" "python-dotenv==1.0.0" "pandas==1.5.3" "tqdm==4.65.0" "uvicorn==0.22.0"
          
          # Verify the installation worked
          python -c "import crawl4ai, pydantic, fastapi; print('Pydantic version:', pydantic.__version__); print('FastAPI version:', fastapi.__version__); print('Dependencies verified successfully')"
          
      # Run Crawl4AI setup
      - name: Run Crawl4AI setup
        run: |
          python -m crawl4ai.cli.setup
          
      # Extract config values
      - name: Extract config values
        id: config
        run: |
          INPUT_FILE=$(python -c "import config; print(config.INPUT_FILE)")
          OUTPUT_FILE=$(python -c "import config; print(config.OUTPUT_FILE)")
          echo "input_file=$INPUT_FILE" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
      # Verify universities.json exists
      - name: Verify universities.json exists
        run: |
          if [ ! -f "${{ steps.config.outputs.input_file }}" ]; then
            echo "ERROR: universities.json file not found at ${{ steps.config.outputs.input_file }}"
            exit 1
          else
            echo "Found universities.json:"
            ls -la data/
          fi
          
      # Create minimal main.py that doesn't require transformers
      - name: Create minimal version of main.py for CI
        run: |
          echo "Creating a CI-specific version of main.py that doesn't require transformers"
          cat > main_ci.py << 'EOF'
          import asyncio
          import json
          import logging
          import time
          import os
          from typing import List, Dict, Any, Optional
          from pydantic import BaseModel, Field
          
          from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
          from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
          from crawl4ai.content_filter_strategy import PruningContentFilter
          import config
          
          # Set up logging
          logging.basicConfig(
              level=logging.INFO,
              format="%(asctime)s [%(levelname)s] %(message)s",
              datefmt="%Y-%m-%d %H:%M:%S"
          )
          
          # Define the extraction schema using Pydantic v2
          class UniversityData(BaseModel):
              """Schema for university data extraction"""
              courses: List[str] = Field(default_factory=list, description="List of courses or programs offered")
              admissions_requirements: List[str] = Field(default_factory=list, description="List of admissions requirements")
              application_deadlines: List[str] = Field(default_factory=list, description="List of application deadlines")
          
          def load_university_urls(filename=config.INPUT_FILE):
              """Loads university URLs dynamically from JSON."""
              try:
                  with open(filename, "r", encoding="utf-8") as f:
                      return json.load(f)
              except Exception as e:
                  logging.error(f"Failed to load universities JSON file: {e}")
                  return []
          
          async def extract_university_data(crawler: AsyncWebCrawler, uni: Dict[str, str]) -> Dict[str, Any]:
              """Extract data from a university website using Crawl4AI."""
              url = uni["url"]
              name = uni["name"]
              
              logging.info(f"Processing {name} at {url}")
              
              # Configure the extraction strategy using CSS since we're in CI
              css_schema = {
                  "courses": {
                      "selector": "div.programs, ul.course-list, .majors, .degrees, .academics",
                      "type": "list"
                  },
                  "admissions_requirements": {
                      "selector": "div.requirements, .admission, .eligibility, ul.requirements",
                      "type": "list"
                  },
                  "application_deadlines": {
                      "selector": "div.deadlines, .dates, table.deadlines, .calendar",
                      "type": "list"
                  }
              }
              
              css_extractor = JsonCssExtractionStrategy(css_schema)
              
              # First try with CSS extractor which is more reliable in CI environments
              run_config_css = CrawlerRunConfig(
                  extraction_strategy=css_extractor,
                  content_filter=PruningContentFilter(),
                  session_id=f"university-{name}-css",
                  cache_mode=CacheMode.PREFER_CACHE,
                  wait_for_selector="body",
                  follow_redirects=True
              )
              
              try:
                  # Use CSS extraction (more reliable in CI environments)
                  logging.info(f"Attempting CSS-based extraction for {name}")
                  result = await crawler.arun(url=url, config=run_config_css)
                  
                  # Check if extraction succeeded with CSS
                  if result.extracted_content and isinstance(result.extracted_content, dict):
                      extracted = result.extracted_content
                      logging.info(f"CSS extraction successful for {name}")
                  else:
                      logging.warning(f"CSS extraction failed for {name}")
                      extracted = {}
                  
                  # Format the data with cleaner structure
                  data = {
                      "name": name,
                      "url": url,
                      "courses": extracted.get("courses", ["Not found"]),
                      "admissions_requirements": extracted.get("admissions_requirements", ["Not found"]),
                      "application_deadlines": extracted.get("application_deadlines", ["Not found"]),
                      "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
                  }
                  
                  # If we got some markdown content but no structured data, we could use that as fallback
                  if all(data[key] == ["Not found"] for key in ["courses", "admissions_requirements", "application_deadlines"]):
                      logging.warning(f"No structured data found for {name}, using markdown fallback")
                      if result.markdown:
                          # Very simple keyword-based extraction from markdown as last resort
                          markdown_lines = result.markdown.split('\n')
                          
                          # Simple keyword matching
                          course_lines = [line.strip() for line in markdown_lines if any(kw in line.lower() for kw in 
                                          ['degree', 'course', 'program', 'major', 'bachelor', 'master', 'phd'])]
                          if course_lines:
                              data["courses"] = course_lines[:5]  # Limit to first 5 matches
                          
                          req_lines = [line.strip() for line in markdown_lines if any(kw in line.lower() for kw in 
                                      ['requirement', 'admission', 'prerequisite', 'qualify', 'eligibility', 'gpa', 'test score'])]
                          if req_lines:
                              data["admissions_requirements"] = req_lines[:5]
                          
                          deadline_lines = [line.strip() for line in markdown_lines if any(kw in line.lower() for kw in 
                                          ['deadline', 'date', 'application period', 'apply by', 'due by', 'submit by'])]
                          if deadline_lines:
                              data["application_deadlines"] = deadline_lines[:5]
                  
                  logging.info(f"Successfully extracted data for {name}")
                  return data
              except Exception as e:
                  logging.error(f"Error extracting data for {name}: {e}")
                  return {
                      "name": name,
                      "url": url,
                      "courses": ["Not found"],
                      "admissions_requirements": ["Not found"],
                      "application_deadlines": ["Not found"],
                      "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                      "error": str(e)
                  }
          
          async def process_universities(universities: List[Dict[str, str]]) -> List[Dict[str, Any]]:
              """Process multiple universities with rate limiting and concurrency control."""
              results = []
              
              # Configure the browser
              browser_config = BrowserConfig(
                  headless=True,  # Run in headless mode
                  ignore_https_errors=True,  # Ignore HTTPS errors
                  timeout=config.DEFAULT_TIMEOUT * 1000,  # Convert to milliseconds
                  user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
              )
              
              # Create connection pool settings based on the number of universities
              # Adjust max_concurrent_tasks based on your system's capabilities
              max_tasks = min(2, len(universities))  # Very conservative setting for CI environment
              
              # Initialize the AsyncWebCrawler with the browser configuration
              async with AsyncWebCrawler(
                  config=browser_config,
                  verbose=True,
                  max_concurrent_tasks=max_tasks
              ) as crawler:
                  # Process universities in batches to control concurrency
                  for i in range(0, len(universities), max_tasks):
                      batch = universities[i:i+max_tasks]
                      logging.info(f"Processing batch {i//max_tasks + 1} with {len(batch)} universities")
                      
                      # Create tasks for the batch
                      tasks = [extract_university_data(crawler, uni) for uni in batch]
                      
                      # Run tasks concurrently
                      batch_results = await asyncio.gather(*tasks)
                      results.extend(batch_results)
                      
                      # Add a small delay between batches to avoid overwhelming the system
                      if i + max_tasks < len(universities):
                          await asyncio.sleep(config.RATE_LIMIT)
              
              return results
          
          async def main():
              # Ensure data directory exists
              os.makedirs(config.DATA_DIR, exist_ok=True)
              
              # Load university data
              universities = load_university_urls()
              if not universities:
                  logging.error("No universities found to scrape. Exiting.")
                  return
              
              logging.info(f"Starting scraping for {len(universities)} universities")
              
              # Validate URLs before processing
              valid_universities = []
              for uni in universities:
                  if not uni.get("url"):
                      logging.warning(f"Missing URL for {uni.get('name', 'unknown university')}, skipping")
                      continue
                      
                  # Ensure URL has proper format
                  url = uni["url"]
                  if not url.startswith(("http://", "https://")):
                      url = "https://" + url
                      uni["url"] = url
                      logging.info(f"Fixed URL format for {uni['name']}: {url}")
                      
                  valid_universities.append(uni)
              
              # Use a smaller sample for CI to make sure it completes faster
              if os.environ.get('CI'):
                  max_unis = min(2, len(valid_universities))
                  logging.info(f"CI environment detected, limiting to {max_unis} universities")
                  valid_universities = valid_universities[:max_unis]
              
              # Extract data
              data = await process_universities(valid_universities)
              
              # Save results
              try:
                  with open(config.OUTPUT_FILE, "w", encoding="utf-8") as f:
                      json.dump(data, f, indent=4)
                  logging.info(f"Saved structured data to {config.OUTPUT_FILE}")
                  
                  # Print a summary
                  found_courses = sum(1 for uni in data if uni["courses"] and uni["courses"][0] != "Not found")
                  found_requirements = sum(1 for uni in data if uni["admissions_requirements"] and uni["admissions_requirements"][0] != "Not found")
                  found_deadlines = sum(1 for uni in data if uni["application_deadlines"] and uni["application_deadlines"][0] != "Not found")
                  
                  logging.info(f"Scraping Summary:")
                  logging.info(f"- Universities processed: {len(data)}/{len(valid_universities)}")
                  logging.info(f"- Found course info: {found_courses}/{len(data)}")
                  logging.info(f"- Found requirements info: {found_requirements}/{len(data)}")
                  logging.info(f"- Found deadline info: {found_deadlines}/{len(data)}")
                  
              except Exception as e:
                  logging.error(f"Error saving data: {e}")
          
          if __name__ == "__main__":
              asyncio.run(main())
          EOF
          
      # Run the scraper with the CI-specific main.py
      - name: Run scraper
        run: |
          # Create data directory if it doesn't exist
          mkdir -p data
          
          # Run the scraper with debug output
          python -u main_ci.py
          
      # Check and display scraping results
      - name: Check and display scraping results
        id: summary
        run: |
          OUTPUT_FILE="${{ steps.config.outputs.output_file }}"
          if [ -f "$OUTPUT_FILE" ]; then
            echo "Scraping results:"
            cat "$OUTPUT_FILE"
            
            # Get university count
            UNI_COUNT=$(python -c "import json; print(len(json.load(open('$OUTPUT_FILE'))))")
            echo "count=$UNI_COUNT" >> $GITHUB_OUTPUT
            
            echo -e "\n\nUniversities successfully scraped:"
            python -c "import json; data = json.load(open('$OUTPUT_FILE')); print('\n'.join([f\"- {uni['name']}\" for uni in data]))"
            
            echo -e "\n\nScraping Statistics:"
            python -c """
            import json
            data = json.load(open('$OUTPUT_FILE'))
            total = len(data)
            if total > 0:
                courses_found = sum(1 for uni in data if uni.get('courses') and uni['courses'][0] != 'Not found')
                req_found = sum(1 for uni in data if uni.get('admissions_requirements') and uni['admissions_requirements'][0] != 'Not found')
                deadlines_found = sum(1 for uni in data if uni.get('application_deadlines') and uni['application_deadlines'][0] != 'Not found')
                print(f'Total universities: {total}')
                print(f'Universities with course info: {courses_found} ({courses_found/total*100:.1f}%)')
                print(f'Universities with requirements info: {req_found} ({req_found/total*100:.1f}%)')
                print(f'Universities with deadline info: {deadlines_found} ({deadlines_found/total*100:.1f}%)')
            else:
                print('No universities were processed')
            """
            echo "Scraping completed successfully."
          else
            echo "Error: No scraping results found in $OUTPUT_FILE"
            exit 1
          fi
          
      # Upload the scraping results as an artifact
      - name: Upload admissions data
        uses: actions/upload-artifact@v4
        with:
          name: admissions-data
          path: ${{ steps.config.outputs.output_file }}
          retention-days: 7
