name: University Admissions Scraper

on:
  # Run on schedule (once a week on Monday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 1'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for running scraper'
        required: false
        default: 'Manual trigger'

jobs:
  process-university-list:
    name: Process University List
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'
          
      - name: Install dependencies for AI agent
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4
          
      - name: Extract config values
        id: config
        run: |
          # Use Python to extract values from config.py
          UNIVERSITIES_LIST_FILE=$(python -c "import config; print(config.UNIVERSITIES_LIST_FILE)")
          INPUT_FILE=$(python -c "import config; print(config.INPUT_FILE)")
          echo "universities_list_file=$UNIVERSITIES_LIST_FILE" >> $GITHUB_OUTPUT
          echo "input_file=$INPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Check for university list
        id: check-list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          if [ -f "$UNIVERSITIES_LIST_FILE" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "List of universities found at $UNIVERSITIES_LIST_FILE"
            echo "First few lines of the file:"
            head -n 5 "$UNIVERSITIES_LIST_FILE"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: No $UNIVERSITIES_LIST_FILE file found. Please create this file with university names and optional fallback URLs."
            exit 1
          fi
      
      - name: Verify AI agent script
        id: check-agent
        run: |
          if [ -f "university_list_processor.py" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "AI agent script found"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: university_list_processor.py not found. Please add the AI agent script to your repository."
            exit 1
          fi
      
      - name: Run AI agent to process university list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          INPUT_FILE="${{ steps.config.outputs.input_file }}"
          mkdir -p data
          python university_list_processor.py --input "$UNIVERSITIES_LIST_FILE" --output "$INPUT_FILE" --debug
          
      - name: Display generated universities.json
        run: |
          echo "Generated universities.json:"
          cat "${{ steps.config.outputs.input_file }}"
          
      # Upload the generated universities.json as an artifact to share between jobs
      - name: Upload universities.json
        uses: actions/upload-artifact@v4
        with:
          name: universities-json
          path: ${{ steps.config.outputs.input_file }}
          retention-days: 1

  scrape-universities:
    name: Scrape University Admissions Pages with Crawl4AI
    needs: process-university-list
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'
          
      # Download the universities.json file generated in the previous job
      - name: Download universities.json
        uses: actions/download-artifact@v4
        with:
          name: universities-json
          path: data
                    
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt
          
      - name: Install Playwright dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            libglib2.0-0 \
            libnss3 \
            libnspr4 \
            libatk1.0-0 \
            libatk-bridge2.0-0 \
            libcups2 \
            libdrm2 \
            libdbus-1-3 \
            libxcb1 \
            libxkbcommon0 \
            libx11-6 \
            libxcomposite1 \
            libxdamage1 \
            libxext6 \
            libxfixes3 \
            libxrandr2 \
            libgbm1 \
            libpango-1.0-0 \
            libcairo2 \
            libasound2 \
            libatspi2.0-0
            
      - name: Run Crawl4AI setup
        run: |
          # Run Crawl4AI setup to install browser binaries
          python -m crawl4ai.cli.setup
          
      - name: Verify dependencies
        run: |
          python -c "import crawl4ai, pydantic; print('Dependencies verified successfully')"
          
      - name: Extract config values
        id: config
        run: |
          # Use Python to extract values from config.py
          INPUT_FILE=$(python -c "import config; print(config.INPUT_FILE)")
          OUTPUT_FILE=$(python -c "import config; print(config.OUTPUT_FILE)")
          echo "input_file=$INPUT_FILE" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Verify universities.json exists
        run: |
          if [ ! -f "${{ steps.config.outputs.input_file }}" ]; then
            echo "ERROR: universities.json file not found at ${{ steps.config.outputs.input_file }}"
            exit 1
          else
            echo "Found universities.json:"
            ls -la data/
          fi
          
      - name: Run scraper
        run: |
          # Create data directory if it doesn't exist
          mkdir -p data
          
          # Run the scraper with debug output
          python -u main.py
          
      - name: Check and display scraping results
        id: summary
        run: |
          OUTPUT_FILE="${{ steps.config.outputs.output_file }}"
          if [ -f "$OUTPUT_FILE" ]; then
            echo "Scraping results:"
            cat "$OUTPUT_FILE"
            
            # Get university count
            UNI_COUNT=$(python -c "import json; print(len(json.load(open('$OUTPUT_FILE'))))")
            echo "count=$UNI_COUNT" >> $GITHUB_OUTPUT
            
            echo -e "\n\nUniversities successfully scraped:"
            python -c "import json; data = json.load(open('$OUTPUT_FILE')); print('\n'.join([f\"- {uni['name']}\" for uni in data]))"
            
            echo -e "\n\nScraping Statistics:"
            python -c """
            import json
            data = json.load(open('$OUTPUT_FILE'))
            total = len(data)
            courses_found = sum(1 for uni in data if uni.get('courses') and uni['courses'][0] != 'Not found')
            req_found = sum(1 for uni in data if uni.get('admissions_requirements') and uni['admissions_requirements'][0] != 'Not found')
            deadlines_found = sum(1 for uni in data if uni.get('application_deadlines') and uni['application_deadlines'][0] != 'Not found')
            print(f'Total universities: {total}')
            print(f'Universities with course info: {courses_found} ({courses_found/total*100:.1f}%)')
            print(f'Universities with requirements info: {req_found} ({req_found/total*100:.1f}%)')
            print(f'Universities with deadline info: {deadlines_found} ({deadlines_found/total*100:.1f}%)')
            """
            echo "Scraping completed successfully."
          else
            echo "Error: No scraping results found in $OUTPUT_FILE"
            exit 1
          fi
          
      # Upload the scraping results as an artifact
      - name: Upload admissions data
        uses: actions/upload-artifact@v4
        with:
          name: admissions-data
          path: ${{ steps.config.outputs.output_file }}
          retention-days: 7
