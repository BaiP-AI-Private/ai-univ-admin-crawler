name: University Admissions Scraper

on:
  # Run on schedule (once a week on Monday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 1'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for running scraper'
        required: false
        default: 'Manual trigger'

jobs:
  scrape:
    name: Scrape University Admissions Pages
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Create requirements-minimal.txt
        run: |
          cat << EOF > requirements-minimal.txt
          charset-normalizer==2.1.1
          multidict==6.0.4
          yarl==1.8.2
          async-timeout==4.0.2
          attrs==23.1.0
          typing-extensions==4.5.0
          idna==3.4
          aiohttp==3.8.5
          bs4==0.0.1
          beautifulsoup4==4.12.2
          soupsieve==2.4.1
          EOF
          
          echo "Created minimal requirements file:"
          cat requirements-minimal.txt
          
      - name: Install minimal dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          python -m pip install -r requirements-minimal.txt
          
      - name: Verify dependencies
        run: |
          python -c "import aiohttp, bs4; print('Dependencies verified successfully')"
          
      - name: Run scraper
        run: |
          # Create data directory if it doesn't exist
          mkdir -p data
          
          # Check if universities.json exists in data directory
          if [ ! -f "data/universities.json" ]; then
            echo "Creating sample universities.json file..."
            echo '[{"name":"Harvard University","url":"https://college.harvard.edu/admissions"},{"name":"MIT","url":"https://www.admissions.mit.edu"}]' > data/universities.json
          fi
          
          # Run the scraper
          python main.py
          
      - name: Check if data was collected
        id: check-data
        run: |
          if [ -f "data/admissions_data.json" ]; then
            FILESIZE=$(stat -c%s "data/admissions_data.json")
            echo "filesize=$FILESIZE" >> $GITHUB_OUTPUT
            if [ "$FILESIZE" -gt 100 ]; then
              echo "valid=true" >> $GITHUB_OUTPUT
            else
              echo "valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "valid=false" >> $GITHUB_OUTPUT
            # Create empty data file to prevent subsequent steps from failing
            echo "[]" > data/admissions_data.json
          fi
          
      - name: Upload data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: admissions-data
          path: data/admissions_data.json
          retention-days: 30
          
  notify:
    name: Notify about scraper results
    needs: scrape
    runs-on: ubuntu-latest
    if: ${{ always() }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Download data artifact
        uses: actions/download-artifact@v4
        with:
          name: admissions-data
          path: ./data
          
      - name: Generate summary
        id: summary
        run: |
          if [ -f "data/admissions_data.json" ]; then
            echo "Universities scraped: $(python -c "import json; print(len(json.load(open('data/admissions_data.json'))))")"
          else
            echo "No data was collected"
          fi

      # Optional: Add email notification
      # - name: Send email notification
      #   if: ${{ always() }}
      #   uses: dawidd6/action-send-mail@v3
      #   with:
      #     server_address: ${{ secrets.MAIL_SERVER }}
      #     server_port: ${{ secrets.MAIL_PORT }}
      #     username: ${{ secrets.MAIL_USERNAME }}
      #     password: ${{ secrets.MAIL_PASSWORD }}
      #     subject: University Scraper Results - ${{ steps.summary.outputs.count }} Universities
      #     to: your-email@example.com
      #     from: GitHub Actions <actions@github.com>
      #     body: |
      #       The university admissions scraper has completed.
      #       ${{ steps.summary.outputs.count }} universities were successfully scraped.
      #       See the latest data in the repository: https://github.com/${{ github.repository }}/blob/main/data/admissions_data.json
      
      # Optional: Add Slack notification
      # - name: Send Slack notification
      #   uses: 8398a7/action-slack@v3
      #   with:
      #     status: ${{ job.status }}
      #     fields: repo,message,workflow,job,commit
      #     text: |
      #       Scraper completed: ${{ steps.summary.outputs.count }} universities were scraped.
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
