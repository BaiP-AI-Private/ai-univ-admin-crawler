name: University Admissions Scraper

on:
  # Run on schedule (once a week on Monday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 1'
  
  # Run when changes are made to key files
  push:
    branches:
      - main
    paths:
      - 'data/list_of_universities.txt'
      - 'university_list_processor.py'
      - 'main.py'
      - '.github/workflows/univ-admin-scraper.yml'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for running scraper'
        required: false
        default: 'Manual trigger'

jobs:
  process-university-list:
    name: Process University List
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Install dependencies for AI agent
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4
          
      - name: Extract config values
        id: config
        run: |
          # Use Python to extract values from config.py
          UNIVERSITIES_LIST_FILE=$(python -c "import config; print(config.UNIVERSITIES_LIST_FILE)")
          echo "universities_list_file=$UNIVERSITIES_LIST_FILE" >> $GITHUB_OUTPUT
          
      - name: Check for university list
        id: check-list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          if [ -f "$UNIVERSITIES_LIST_FILE" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "List of universities found at $UNIVERSITIES_LIST_FILE"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: No $UNIVERSITIES_LIST_FILE file found. Please create this file with a list of universities, one per line."
            exit 1
          fi
      
      - name: Verify AI agent script
        id: check-agent
        run: |
          if [ -f "university_list_processor.py" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "AI agent script found"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "ERROR: university_list_processor.py not found. Please add the AI agent script to your repository."
            exit 1
          fi
      
      - name: Run AI agent to process university list
        run: |
          UNIVERSITIES_LIST_FILE="${{ steps.config.outputs.universities_list_file }}"
          mkdir -p data
          python university_list_processor.py --input "$UNIVERSITIES_LIST_FILE" --output data/universities.json
          
      - name: Display generated universities.json
        run: |
          echo "Generated universities.json:"
          cat data/universities.json
          
      - name: Upload universities.json as artifact
        uses: actions/upload-artifact@v4
        with:
          name: universities-data
          path: data/universities.json
          retention-days: 7

  scrape-universities:
    name: Scrape University Admissions Pages
    needs: process-university-list
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Download universities.json artifact
        uses: actions/download-artifact@v4
        with:
          name: universities-data
          path: data
          
      - name: Install dependencies for scraper
        run: |
          python -m pip install --upgrade pip
          pip install charset-normalizer multidict yarl async-timeout attrs idna typing-extensions
          pip install aiohttp beautifulsoup4
          
          # Install other dependencies if needed
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt --no-deps || pip install -r requirements.txt
          fi
          
      - name: Run university scraper
        run: |
          # Verify data directory and universities.json exist
          mkdir -p data
          if [ ! -f "data/universities.json" ]; then
            echo "Error: universities.json not found!"
            exit 1
          fi
          
          # Run the scraper
          python main.py
          
      - name: Check if data was collected
        id: check-data
        run: |
          if [ -f "data/admissions_data.json" ]; then
            FILESIZE=$(stat -c%s "data/admissions_data.json")
            echo "filesize=$FILESIZE" >> $GITHUB_OUTPUT
            if [ "$FILESIZE" -gt 100 ]; then
              echo "valid=true" >> $GITHUB_OUTPUT
            else
              echo "valid=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "valid=false" >> $GITHUB_OUTPUT
            # Create empty data file to prevent subsequent steps from failing
            echo "[]" > data/admissions_data.json
          fi
          
      - name: List found universities
        if: steps.check-data.outputs.valid == 'true'
        run: |
          echo "Universities data successfully collected:"
          python -c "import json; data = json.load(open('data/admissions_data.json')); print('\n'.join([f\"- {uni['name']}\" for uni in data]))"
          
      - name: Commit and push if data changed
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          git add data/admissions_data.json
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update admissions data [skip ci]" && git push)
          
      - name: Upload admissions data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: admissions-data
          path: data/admissions_data.json
          retention-days: 30
          
  notify:
    name: Notify about scraper results
    needs: scrape-universities
    runs-on: ubuntu-latest
    if: ${{ always() }}
    
    steps:
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Download admissions data artifact
        uses: actions/download-artifact@v4
        with:
          name: admissions-data
          path: ./data
          
      - name: Generate summary
        id: summary
        run: |
          if [ -f "data/admissions_data.json" ]; then
            COUNT=$(python -c "import json; print(len(json.load(open('data/admissions_data.json'))))")
            echo "Universities scraped: $COUNT"
            echo "count=$COUNT" >> $GITHUB_OUTPUT
          else
            echo "No data was collected"
            echo "count=0" >> $GITHUB_OUTPUT
          fi

      # Optional: Add email notification
      # - name: Send email notification
      #   if: ${{ always() }}
      #   uses: dawidd6/action-send-mail@v3
      #   with:
      #     server_address: ${{ secrets.MAIL_SERVER }}
      #     server_port: ${{ secrets.MAIL_PORT }}
      #     username: ${{ secrets.MAIL_USERNAME }}
      #     password: ${{ secrets.MAIL_PASSWORD }}
      #     subject: University Scraper Results - ${{ steps.summary.outputs.count }} Universities
      #     to: your-email@example.com
      #     from: GitHub Actions <actions@github.com>
      #     body: |
      #       The university admissions scraper has completed.
      #       ${{ steps.summary.outputs.count }} universities were successfully scraped.
      #       See the latest data in the repository: https://github.com/${{ github.repository }}/blob/main/data/admissions_data.json
      
      # Optional: Add Slack notification
      # - name: Send Slack notification
      #   uses: 8398a7/action-slack@v3
      #   with:
      #     status: ${{ job.status }}
      #     fields: repo,message,workflow,job,commit
      #     text: |
      #       Scraper completed: ${{ steps.summary.outputs.count }} universities were scraped.
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
