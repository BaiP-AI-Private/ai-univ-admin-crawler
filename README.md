# **Deep Seek Crawler - Kubernetes Deployment Guide** ğŸš€

This project is a scalable web crawler built with Python that extracts **university admission and course data** asynchronously using Crawl4AI. It utilizes a **BERT-powered AI model** for CSS selector detection and Reinforcement Learning to refine predictions dynamically.

## **Features**
âœ… Asynchronous web crawling with [Crawl4AI](https://pypi.org/project/Crawl4AI/)  
âœ… AI-powered CSS selector detection using BERT  
âœ… Reinforcement Learning for improving accuracy over time  
âœ… JSON-based university configuration for easy updates  
âœ… Kubernetes deployment for **scalability**

---

## **Project Structure**
```
.
â”œâ”€â”€ main.py                 # Web scraper using AI-powered CSS selector detection
â”œâ”€â”€ api.py                  # FastAPI-based API for real-time predictions
â”œâ”€â”€ config.py               # Global settings (timeouts, selectors, reward values)
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ selector_agent.py   # AI model for CSS selector detection using Reinforcement Learning
â”‚   â”œâ”€â”€ rewards.py          # Defines reward system for AI model training
â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ data_utils.py       # Utility functions for saving extracted data
â”‚   â”œâ”€â”€ scraper_utils.py    # Web scraping utility functions
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ universities.json   # List of university URLs for the scraper
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ deployment.yaml     # Kubernetes deployment file
â”‚   â”œâ”€â”€ service.yaml        # Kubernetes service file
â”‚   â”œâ”€â”€ helm-chart/         # Helm chart for easier deployment
â”œâ”€â”€ Dockerfile              # Containerization setup
â”œâ”€â”€ requirements.txt        # Python dependencies (Also at root)
â”œâ”€â”€ .env                    # Environment variables (API keys)
â”œâ”€â”€ .gitignore              # Ignores env & compiled files
â””â”€â”€ README.md               # Deployment instructions

```

## **Installation Steps**
### **1ï¸âƒ£ Create & Activate a Conda Environment**
```bash
conda create -n deep-seek-crawler python=3.12 -y
conda activate deep-seek-crawler
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Set Up Environment Variables**
Create a `.env` file in the root directory:
```
GROQ_API_KEY=your_groq_api_key_here
```
*(Note: The `.env` file is included in `.gitignore`, so it wonâ€™t be pushed to version control.)*

---

## **ğŸš€ Running the Web Scraper**
```bash
python main.py
```
This script extracts **course listings and admissions information** from university websites dynamically.

---

## **ğŸš€ Running the AI-Powered Prediction API**
Start the FastAPI API that predicts CSS selectors dynamically:
```bash
uvicorn api:app --host 0.0.0.0 --port 8000
```

### **Example API Request**
```python
import requests

html_data = "<html><body><div class='course-list'>Machine Learning</div></body></html>"
response = requests.post("http://127.0.0.1:8000/predict", json={"html": html_data})
print(response.json())  # Returns predicted CSS selector
```

---

## **ğŸš€ Deployment with Kubernetes**
### **1ï¸âƒ£ Build the Docker Image**
```bash
docker build -t deepseek-crawler .
```

### **2ï¸âƒ£ Push to Docker Hub**
Replace `your-docker-username` with your Docker Hub username:
```bash
docker tag deepseek-crawler your-docker-username/deepseek-crawler:v1
docker push your-docker-username/deepseek-crawler:v1
```

### **3ï¸âƒ£ Apply Kubernetes Deployment**
Ensure your Kubernetes cluster is running (`kubectl get nodes`):
```bash
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

### **4ï¸âƒ£ Verify Deployment**
```bash
kubectl get pods
kubectl get services
```

---

## **ğŸš€ Scaling with Helm**
For easier deployments, use **Helm charts**:

### **1ï¸âƒ£ Install Helm**
```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```

### **2ï¸âƒ£ Deploy using Helm**
```bash
helm install deepseek-crawler ./k8s/helm-chart/
```

### **3ï¸âƒ£ Verify Pods**
```bash
kubectl get pods
```

---

## **License**
MIT License (or update with the appropriate license)
